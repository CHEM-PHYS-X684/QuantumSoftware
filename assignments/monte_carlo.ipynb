{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7021fa8",
   "metadata": {},
   "source": [
    "# Ising Model \n",
    "For a graph, $G = (E,V)$, defined by a set of edges, $E$, and vertices, $V$, we want to represent an Ising model, where the edge weights, $w_{ij}$ are given by the spin interactions, i.e., $w_{ij} = J_{ij}$.\n",
    "\n",
    "Given a configuration of spins (e.g., $\\uparrow\\downarrow\\downarrow\\uparrow\\downarrow$) we can define the energy using what is referred to as an Ising Hamiltonian:\n",
    "$$ \\hat{H} = \\sum_{(i,j)\\in E}J_{ij} s_is_j + \\sum_i \\mu_i s_i.$$\n",
    "where, $s_i=1$ if the $i^{th}$ spin is `up` and $s_i=-1$ if it is `down`, and the sumation runs over all edges in the graph. \n",
    "**Note:** As we saw before, this Hamiltonian operator is simple, in that a single `BitString` returns a single energy. This is because the matrix representation of the Hamiltonian operator in the computational basis (i.e., basis of all possible `BitString`'s) is a diagonal matrix. However, most quantum mechanical Hamiltonians will not be diagonal, and in that case applying $H$ to a single `BitString` would generate multiple `BitString`'s.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea117c0",
   "metadata": {},
   "source": [
    "\n",
    "# Thermodynamic averages\n",
    "In the previous notebook, we used the Hamiltonian (which was defined as a graph) to find the lowest \"energy\" configuration (`BitString`). However, often times we want to compute average values of an observable over all possible configurations. Imagine that you have a bag containing some mixture of `BitString`'s. If we reach into the back and pull out a `BitString` at random, the probability of observing the specific `BitString` $\\ket{\\alpha}$ will be denoted as $P(\\alpha)$. Each possible `BitString` has its own probability. \n",
    "\n",
    "Given this situation, what is the average energy in the bag? To answer this, we could just pull out each `BitString`, measure it's energy, add them all up, and divide by the total number of `BitString`s. Or if we knew the probabilty of observing each possible `BitString`, we could equivalently, add up the probabilities times the energy, $E(\\alpha)$, of each possible `BitString`:\n",
    "$$\n",
    "\\left<E\\right> = \\sum_\\alpha P(\\alpha)E(\\alpha)\n",
    "$$ \n",
    "In this sense, the average energy (or any average quantity) depends on the given probability distribution in the bag. \n",
    "\n",
    "While there are an infinite number of possible probability distributions one might interact with, a very common distribution (and the one we will focus on) is the `Gibbs Distribution`, also called the `Boltzmann Distribution`:\n",
    "$$\n",
    "P(\\alpha) = \\frac{e^{-\\beta E(\\alpha)}}{Z} = \\frac{e^{-\\beta E(\\alpha)}}{\\sum_{\\alpha'}e^{-\\beta E(\\alpha')}}\n",
    "$$\n",
    "where $\\beta$ sometimes has a physical meaning of $\\beta = 1/kT$, where $k$ is the Boltzmann constant, $k = 1.38064852 \\times 10^{-23} J/K$ and $T$ is the temperature in Kelvin. We generally refer to the normalization constant $Z$ as the partition function. \n",
    "\n",
    "This expression, defines the probability of observing a particular configuration of spins, $\\alpha$. As you can see, the probability of pulling $\\alpha$ out of your bag decays exponentially with increasing energy of $\\alpha$, $E(\\alpha)$. This expression governs the behavior of the vast majority of physical systems, meaning that in nature at low temperatures, one typically expects to observe the lowest possible configuration of a system.\n",
    "\n",
    "If the population (e.g., the bag of `BitString`s) is known to form a Boltzmann distribution, the expectation value of any quantity, `A`, can be defined as:\n",
    "$$\n",
    "\\left<A\\right> = \\frac{\\sum_\\alpha e^{-\\beta E(\\alpha)}A(\\alpha)}{Z}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354ba7ae",
   "metadata": {},
   "source": [
    "# Properties\n",
    "\n",
    "For any fixed state, $\\alpha$, the `magnetization` ($M$) is proportional to the _excess_ number of spins pointing up or down while the energy is given by the\n",
    "Hamiltonian:\n",
    "$$M(\\alpha) = N_{\\text{up}}(\\alpha) - N_{\\text{down}}(\\alpha).$$\n",
    "As a dynamical, fluctuating system, each time you measure the magnetization, the system might be in a different state ($\\alpha$) and so you'll get a different number!\n",
    "However, we already know what the probability of measuring any particular $\\alpha$ is, so in order to compute the average magnetization, $\\left<M\\right>$, we just need to multiply the magnetization of each possible configuration times the probability of it being measured, and then add them all up!\n",
    "$$ \\left<M\\right> = \\sum_\\alpha M(\\alpha)P(\\alpha).$$\n",
    "In fact, any average value can be obtained by adding up the value of an individual configuration multiplied by it's probability:\n",
    "$$ \\left<E\\right> = \\sum_\\alpha E(\\alpha)P(\\alpha).$$\n",
    "\n",
    "This means that to exactly obtain any average value (also known as an `expectation value`) computationally, we must compute the both the value and probability of all possible configurations. This becomes extremely expensive as the number of spins ($N$) increases.\n",
    "\n",
    "The expectation values we will compute in this notebook are\n",
    "\\begin{align}\n",
    "\\text{Energy} &= \\left<E\\right> \\\\\n",
    "\\text{Magnetization} &= \\left<M\\right> \\\\\n",
    "\\text{Heat Capacity} &= \\left(\\left<E^2\\right>-\\left<E\\right>^2\\right)T^{-2} \\\\\n",
    "\\text{Magnetic Susceptibility} &= \\left(\\left<M^2\\right>-\\left<M\\right>^2\\right)T^{-1}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82c23fb",
   "metadata": {},
   "source": [
    "# Expectation values for Boltzmann Distribution\n",
    "\n",
    "In this notebook, we will write code to compute the expectation values of a few different properties, at a given temperature. We will then see how these change with temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf42490",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4d193f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import montecarlo\n",
    "import graphbuilder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e02dd",
   "metadata": {},
   "source": [
    "## Create a graph that defines the Ising interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69405be",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = graphbuilder.build_graph2()\n",
    "N = G.number_of_edges()\n",
    "\n",
    "# Now Draw the graph. \n",
    "plt.figure(1)\n",
    "nx.draw(G, with_labels=True, font_weight='bold')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47889970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new configuration instance for a 6-site lattice\n",
    "conf = montecarlo.BitString(N)\n",
    "ham = montecarlo.IsingHamiltonian(G)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895cd186-2e40-48c6-9f3c-b67104321bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the average values for Temperature = 1\n",
    "E, M, HC, MS = ham.compute_average_values(1)\n",
    "\n",
    "\n",
    "print(\" E  = %12.8f\" %E)\n",
    "print(\" M  = %12.8f\" %M)\n",
    "print(\" HC = %12.8f\" %HC)\n",
    "print(\" MS = %12.8f\" %MS)\n",
    "\n",
    "assert(np.isclose(E,  -11.95991923))\n",
    "assert(np.isclose(M,   -0.00000000))\n",
    "assert(np.isclose(HC,   0.31925472))\n",
    "assert(np.isclose(MS,   0.01202961))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a61e1c-d89d-412d-a5f0-4f7d04eab8a5",
   "metadata": {},
   "source": [
    "# Properties vs Temperature (exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54071e30-5d52-475c-a5ff-f327bb632286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists that we will fill with the property vs. temperature data\n",
    "e_list = []\n",
    "e2_list = []\n",
    "m_list = []\n",
    "m2_list = []\n",
    "T_list = []\n",
    "\n",
    "for Ti in range(1,100):\n",
    "    T = .1*Ti\n",
    "    \n",
    "    E, M, HC, MS = ham.compute_average_values(T)\n",
    "    \n",
    "    e_list.append(E)\n",
    "    m_list.append(M)\n",
    "    e2_list.append(HC)\n",
    "    m2_list.append(MS)\n",
    "    T_list.append(T)\n",
    "\n",
    "\n",
    "plt.plot(T_list, e_list, label=\"energy\");\n",
    "plt.plot(T_list, m_list, label=\"magnetization\");\n",
    "plt.plot(T_list, m2_list, label=\"Susceptibility\");\n",
    "plt.plot(T_list, e2_list, label=\"Heat Capacity\");\n",
    "plt.legend();\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.ylabel(\"Property\")\n",
    "\n",
    "\n",
    "Tc_ind = np.argmax(m2_list)\n",
    "print(\" Critical Temperature: %12.8f \" %(T_list[Tc_ind]))\n",
    "print(\"     E:  %12.8f\" %(e_list[Tc_ind]))\n",
    "print(\"     M:  %12.8f\" %(m_list[Tc_ind]))\n",
    "print(\"     HC: %12.8f\" %(e2_list[Tc_ind]))\n",
    "print(\"     MS: %12.8f\" %(m2_list[Tc_ind]))\n",
    "Tc2 = T_list[np.argmax(e2_list)]\n",
    "print(\" Critical Temperature: %12.8f\" %(Tc2))\n",
    "\n",
    "print(\" E = %12.8f @ T = %12.8f\"% (e_list[T_list.index(2.00)], 2.0))\n",
    "\n",
    "assert(np.isclose(Tc2, 2.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97b02a9-37ea-4f6c-b59b-cc0103181da6",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b448aa86",
   "metadata": {},
   "source": [
    "# Metropolis Sampling\n",
    "\n",
    "These exact calculations pose a drastic problem from a practical perspective. As `N` becomes large it is evident that computations become a daunting task if calculated in this manner. Thus a better numerical alternative would be to use a simulation to generate data over the ‘representative states’. Because each computed value of a configuration gets weighted by it's probability, very high energy states (which have negligible probabilities due to the Boltzmann distribution) will not contribute to the overall sum. What we'd like to do instead, is to sample over *only* those configurations that actively contribute. This is a form of biased sampling which essentially boils down to satisfying the following condition where\n",
    "\n",
    "**generated frequency = actual probability**.\n",
    "\n",
    "We could choose to randomly `sample` from this full set of configurations, but this will not converge quickly. By randomly sampling, we simply mean to pick a configuration, $s_i$, at random, where all configurations have equal probability of being selected. However, what if we *knew* a priori what the equilibrium probability distribution was? Then we could select configurations not randomly, but rather where a given configuration's probability of being selected was proportional to its equilibrium population,\n",
    "\n",
    "$$\n",
    "\\frac{1}{Z}e^{-E(s_i)/T}.\n",
    "$$\n",
    "\n",
    "If each configuration had the thermodynamic probability of being sampled, then our average value would reduce to a simple arithmetic average over the samples:\n",
    "\n",
    "$$\n",
    "\\left<A\\right> = \\frac{1}{M} \\sum_{s_i}^{\\in \\text{Trajectory}} A(s_i).\n",
    "$$\n",
    "\n",
    "How can we carry out this fancy, biased sampling? - we can use a Markov process! We will *walk* randomly through configuration space, where the chance of take a particular step depends only on our current \"position\". For each configuration that we visit, we will compute properties that contribute to our average values. From our current configuration, we will propose a new randomly chosen configuration and decide whether or not to visit that new configuration (i.e. should we accept the proposed step). The rules we use to decide to visit a new configuration will be chosen to guarantee that the number of times we visit a configuration is exactly proportional to the equilibrium population of that configuration!\n",
    "\n",
    "The choice to visit a new configuration will be made probabilistically. Assume we are currently visiting configuration $s_i$. We select configuration $s_j$ at random, and want to decide whether or not to visit $s_j$. If the $s_j$ is lower in energy than $s_i$, then we will visit the new configuration with 100% probability, \n",
    "\n",
    "$$\n",
    "W(s_i\\rightarrow s_j)=1.\n",
    "$$\n",
    "\n",
    "If the $s_j$ is *higher* in energy, then we will visit the new configuration with a probability given by,\n",
    "\n",
    "$$\n",
    "W(s_i\\rightarrow s_j)=e^{-\\left(E(s_j)-E(s_i)\\right)/T}.\n",
    "$$\n",
    "\n",
    "This seems simple, but it's quite powerful! We want to use this to make our simulations much faster, at the cost of some statistical noise.\n",
    "\n",
    "```\n",
    "Initialize configuration, i \n",
    "Loop over Monte Carlo steps\t    \n",
    "\tLoop over sites, n\n",
    "\t\tPropose new configuration, j, by flipping site, n.\n",
    "\t\tCompute flipping probability, W(i→j). \n",
    "\t\tIf  W(i→j) is greater than a randomly chosen number between 0 and 1, \n",
    "\t\t\tAccept (i = j), \n",
    "\t\telse: \n",
    "\t\t\tReject \n",
    "\tUpdate average values with updated i\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc58ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "# Initialize BitString\n",
    "ham = montecarlo.IsingHamiltonian(G).set_mu([.1 for i in range(N)])\n",
    "\n",
    "# Set Temperature \n",
    "T = 2 \n",
    "\n",
    "# Run exact average values for comparison\n",
    "Eref, Mref, HCref, MSref = ham.compute_average_values(T)\n",
    "\n",
    "# Run montecarlo\n",
    "mc = montecarlo.MonteCarlo(ham)\n",
    "E, M = mc.run(T=T, n_samples=100000, n_burn=100)\n",
    "\n",
    "\n",
    "Eavg = np.mean(E)\n",
    "Estd = np.std(E)\n",
    "Mavg = np.mean(M)\n",
    "Mstd = np.std(M)\n",
    "\n",
    "HC = (Estd**2)/(T**2)\n",
    "MS = (Mstd**2)/T\n",
    "\n",
    "print(\"     E:  %12.8f E(ref):  %12.8f error: %12.2e \" %(Eavg, Eref, Eavg - Eref))\n",
    "print(\"     M:  %12.8f M(ref):  %12.8f error: %12.2e \" %(Mavg, Mref, Mavg - Mref))\n",
    "print(\"     HC: %12.8f HC(ref): %12.8f error: %12.2e \" %(HC, HCref, HC - HCref))\n",
    "print(\"     MS: %12.8f MS(ref): %12.8f error: %12.2e \" %(MS, MSref, MS - MSref))\n",
    "\n",
    "\n",
    "def running_average(data):\n",
    "    N = len(data)\n",
    "    r_avg = np.zeros(N)\n",
    "\n",
    "    if N == 0:\n",
    "        return r_avg\n",
    "\n",
    "    r_avg[0] = data[0]\n",
    "    for i in range(1, N):\n",
    "        r_avg[i] = (r_avg[i - 1] * (i) + data[i]) / (i + 1)\n",
    "    \n",
    "    return r_avg\n",
    "\n",
    "plt.plot(running_average(E), label=\"energy\");\n",
    "plt.plot([Eref]*len(E), label=\"exact\");\n",
    "plt.legend();\n",
    "# plt.ylim(-12, -9);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d89764",
   "metadata": {},
   "source": [
    "# Energy/Magnetization vs Temperature (Metropolis Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3338b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_T_scan(ham, conf, Tstep=.1, Tmax=10, n_mc_steps=2000, n_burn=200):\n",
    "    \n",
    "    N = len(conf.config)\n",
    "    if len(ham.J) != N:\n",
    "        error(\"dimensionMismatch\")\n",
    "\n",
    "    T_range = []\n",
    "    e_vs_T = []\n",
    "    m_vs_T = []\n",
    "    e_vs_T_err = []\n",
    "    m_vs_T_err = []\n",
    "    heat_cap_vs_T = []\n",
    "    magn_sus_vs_T = []\n",
    "    \n",
    "    mc = montecarlo.MonteCarlo(ham)\n",
    "    T = 1*Tstep\n",
    "    for Ti in range(int(Tmax/Tstep)):\n",
    "        T += Tstep\n",
    "        \n",
    "        E, M = mc.run(T=T, n_samples=n_mc_steps, n_burn=n_burn)\n",
    "\n",
    "        Eavg = np.mean(E)\n",
    "        Estd = np.std(E)\n",
    "        Mavg = np.mean(M)\n",
    "        Mstd = np.std(M)\n",
    "\n",
    "        HC = (Estd**2)/(T**2)\n",
    "        MS = (Mstd**2)/T\n",
    "        \n",
    "        T_range.append(T)\n",
    "\n",
    "        e_vs_T.append(Eavg)\n",
    "        e_vs_T_err.append(Estd/np.sqrt(len(E))*3)\n",
    "        m_vs_T.append(Mavg)\n",
    "        m_vs_T_err.append(Mstd/np.sqrt(len(M))*3)\n",
    "\n",
    "        heat_cap_vs_T.append(HC)\n",
    "        magn_sus_vs_T.append(MS)\n",
    "\n",
    "\n",
    "    # plt.plot(T_range,e_vs_T, label=\"Energy\")\n",
    "    plt.errorbar(T_range,e_vs_T,yerr=e_vs_T_err, fmt='-', label=\"Energy\")\n",
    "    plt.errorbar(T_range,m_vs_T,yerr=m_vs_T_err, fmt='-', label=\"Magnetization\")\n",
    "    # plt.plot(T_range,m_vs_T, label=\"Magnetization\")\n",
    "    plt.plot(T_range,magn_sus_vs_T, label=\"Susceptibility\")\n",
    "    plt.plot(T_range,heat_cap_vs_T, label=\"Heat Capacity\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d408b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_T_scan(ham, conf, Tstep=.1, Tmax=10, n_mc_steps=2000, n_burn=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "x684_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
